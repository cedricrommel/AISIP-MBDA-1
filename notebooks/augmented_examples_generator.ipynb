{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6acbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nilearn.datasets import fetch_neurovault\n",
    "from nibabel import Nifti1Image\n",
    "from nilearn import plotting\n",
    "from torchio import transforms\n",
    "import torchio as tio\n",
    "\n",
    "from ai4sipmbda.utils import fetching, difumo_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b797594b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 images found on local disk.\n"
     ]
    }
   ],
   "source": [
    "data = fetching.fetch_nv(\"/storage/store2/data/\", max_images=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a49f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b751aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_labels(\n",
    "    data: sklearn.utils.Bunch,\n",
    "    study: str = \"hcp\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Fetches the relavant metadata from json files contained in base_path and\n",
    "    creates labels in format compatible with the training pipeline\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : sklearn.utils.Bunch\n",
    "        \n",
    "    study : str, optional\n",
    "        by default \"hcp\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing the subjects and contrast as used in the training\n",
    "        scripts.\n",
    "    \"\"\"\n",
    "\n",
    "    Y = list()\n",
    "    for metadata in data[\"images_meta\"]:\n",
    "        Y.append({\n",
    "            \"study\": study,\n",
    "            \"subject\": metadata[\"name\"].split(\"_\")[0],\n",
    "            \"contrast\": metadata[\"contrast_definition\"],\n",
    "            #\"meta_path\": os.path.join(data_dir, fname),\n",
    "            # \"path\": os.path.join(data_dir, img_filename),\n",
    "        })\n",
    "    return pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed67685",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_dataset_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443aee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENTATIONS = {\n",
    "        \"RandomElasticDeformation\": tio.RandomElasticDeformation(num_control_points = 16, max_displacement = 2),\n",
    "        \"RandomMotion\":tio.RandomMotion(degrees = 0.2, translation = 0.2, num_transforms = 2),\n",
    "        \"RandomGhosting\": tio.RandomGhosting(num_ghosts = 1, intensity = 0.02, restore = 1.0),\n",
    "        \"RandomSpike\": tio.RandomSpike(num_spikes = 2, intensity = 1.15),\n",
    "        \"RandomBiasField\": tio.RandomBiasField(order = 1,coefficients=0.05 ),\n",
    "        \"RandomBlur\": tio.RandomBlur(std = 1.05),\n",
    "        \"RandomNoise\":tio.RandomNoise(mean = 0.3, std = 0.5),\n",
    "        \"RandomGamma\": tio.RandomGamma(log_gamma=0.075),\n",
    "        \"RandomFlip\": tio.RandomFlip(flip_probability=1.0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "237c6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmentation(\n",
    "    aug_names: List[str],\n",
    ") -> transforms.Transform:\n",
    "    augmentation_list = [AUGMENTATIONS[aug] for aug in aug_names]\n",
    "    return tio.transforms.OneOf(augmentation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f624199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flip = create_augmentation([\"RandomFlip\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e75effd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_inv = np.load(\"hcp900_difumo_matrices/Zinv.npy\")\n",
    "mask = np.load(\"hcp900_difumo_matrices/mask.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a783974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b967349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hcp' '106521' '0BK_PLACE']\n",
      "['hcp' '117324' 'RF']\n",
      "['hcp' '107321' 'SHAPES']\n",
      "['hcp' '165638' '0BK_FACE']\n",
      "['hcp' '465852' '0BK_TOOL']\n",
      "['hcp' '124624' '0BK_FACE']\n",
      "['hcp' '119126' 'RANDOM']\n",
      "['hcp' '208226' 'RH']\n",
      "['hcp' '395756' 'TOM']\n",
      "['hcp' '105923' '0BK_TOOL']\n"
     ]
    }
   ],
   "source": [
    "for task in labels.values:\n",
    "    print(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94f65005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_based_augmentation(augmentation, mask, Z_inv, images_paths, labels, nb_fakes=10, n_jobs=1, verbose=0):\n",
    "\n",
    "    def _create_fakes(image_path, task):\n",
    "        print(f\"Starting to augment {image_path}\")\n",
    "\n",
    "        image_tio = tio.ScalarImage(image_path)\n",
    "\n",
    "        sub_X = [Z_inv.dot(image_tio.data.squeeze()[mask])]\n",
    "\n",
    "        for _ in range(nb_fakes):\n",
    "            # transform\n",
    "            trf_img_tio = augmentation(image_tio)\n",
    "\n",
    "            # project\n",
    "            trf_difumo_vec = Z_inv.dot(trf_img_tio.data.squeeze()[mask])\n",
    "\n",
    "            sub_X.append(trf_difumo_vec)\n",
    "        print(f\"Finished to augment {image_path}\")\n",
    "        return np.vstack(sub_X), np.vstack([task] * (1 + nb_fakes))\n",
    "\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=verbose)\n",
    "    ret = parallel(delayed(_create_fakes)(image_path, task) for image_path, task in zip(images_paths, labels))\n",
    "\n",
    "    X, Y = zip(*ret)\n",
    "\n",
    "    return np.vstack(X), pd.DataFrame(np.vstack(Y), columns=labels.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06b3090e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    3.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    5.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    5.5s finished\n"
     ]
    }
   ],
   "source": [
    "augmented_X, Y = transform_based_augmentation(flip, mask, Z_inv, data[\"images\"], labels=labels, nb_fakes=2, n_jobs=3, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c5691e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 1024)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92623f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_augs = create_augmentation(list(AUGMENTATIONS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3aa57174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    7.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    7.2s finished\n"
     ]
    }
   ],
   "source": [
    "augmented_X = transform_based_augmentation(all_augs, mask, Z_inv, data[\"images\"], Y=None, nb_fakes=10, n_jobs=3, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b17ef96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1024)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47350a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib.parallel import Parallel, delayed\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f9ba762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_label(Y_t, use_dict=None, return_dict=False):\n",
    "    \"\"\"\n",
    "    Preprocess label so that they match classifier like format\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_t: pandas DataFrame\n",
    "        labels to preprocess\n",
    "    use_dict: dictionary\n",
    "        If a dictionary labels -> class number is already known\n",
    "    return_dict: bool\n",
    "        If true, returns the dictionary labels -> class number\n",
    "    Returns\n",
    "    --------\n",
    "    Y: np array\n",
    "        np array where each label is replaced by its class number\n",
    "    dict (optional): dict\n",
    "        dictionary label -> class number (only returned if return_dict is True)\n",
    "    \"\"\"\n",
    "    if use_dict is None:\n",
    "        Y_dict = {v: k for k, v in enumerate(Y_t[\"contrast\"].unique())}\n",
    "    else:\n",
    "        Y_dict = use_dict\n",
    "\n",
    "    if return_dict:\n",
    "        return Y_t[\"contrast\"].apply(lambda x: Y_dict[x]).values, Y_dict\n",
    "    else:\n",
    "        return Y_t[\"contrast\"].apply(lambda x: Y_dict[x]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce8ba733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model, f):\n",
    "        \"\"\"\n",
    "        Trains a classifier using an augmentation method.\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: BaseEstimator\n",
    "            The classifier used\n",
    "        f: function (X, Y) -> X_fake, Y_fake\n",
    "            The data augmentation function that\n",
    "            generates fake (labeled) data from\n",
    "            input data\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "        self.f = f\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        if self.f is None:\n",
    "            return self.model.fit(X, Y)\n",
    "        else:\n",
    "            X_fake, Y_fake = self.f(X, Y)\n",
    "            return self.model.fit(\n",
    "                np.row_stack([X_fake, X]), np.concatenate([Y_fake, Y])\n",
    "            )\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        return self.model.predict(X, y)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1de608a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_classif(\n",
    "    images_path, labels, f, method_name, filename, train_size, n_splits=5, n_jobs=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Tries 4 different classifier with the given augmentation method\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np array of shape (n_samples, n_features)\n",
    "        Input data\n",
    "    Y: np array of shape (n_samples,)\n",
    "        Labels\n",
    "    f: function of X, Y\n",
    "        f returns fake data and fake labels\n",
    "    method_name: str\n",
    "        the name of the method used to produce fake data\n",
    "    filename: str\n",
    "        filename is the path result file\n",
    "    train_size : float or int, default=None\n",
    "        If float, should be between 0.0 and 1.0 and represent the\n",
    "        proportion of the dataset to include in the train split. If\n",
    "        int, represents the absolute number of train samples. If None,\n",
    "        the value is automatically set to the complement of the test size.\n",
    "    n_splits : int, default=10\n",
    "        Number of re-shuffling & splitting iterations.\n",
    "    n_jobs: int, default: None\n",
    "        The maximum number of concurrently running jobs,\n",
    "    \"\"\"\n",
    "\n",
    "    models = []\n",
    "    # parameters for MLP only\n",
    "    params_2L = {\n",
    "        \"activation\": \"relu\",\n",
    "        \"solver\": \"adam\",\n",
    "        \"learning_rate\": \"constant\",\n",
    "        \"momentum\": 0.9,\n",
    "        \"learning_rate_init\": 0.0001,\n",
    "        \"alpha\": 0.00001,\n",
    "        \"random_state\": 0,\n",
    "        \"batch_size\": 32,\n",
    "        \"hidden_layer_sizes\": (1024, 1024),\n",
    "        \"max_iter\": 20000,\n",
    "    }\n",
    "\n",
    "    models.append(\n",
    "        (LinearDiscriminantAnalysis(solver=\"lsqr\", shrinkage=\"auto\"), \"LDA\")\n",
    "    )\n",
    "    models.append((RandomForestClassifier(verbose=True), \"RF\"))\n",
    "    # models.append((MLPClassifier(verbose=True, **params_2L), \"MLP\"))\n",
    "    # models.append(\n",
    "    #     (\n",
    "    #         GridSearchCV(\n",
    "    #             LogisticRegression(\n",
    "    #                 solver=\"lbfgs\",\n",
    "    #                 tol=1e-4,\n",
    "    #                 random_state=11,\n",
    "    #                 penalty=\"l2\",\n",
    "    #                 max_iter=20000,\n",
    "    #                 n_jobs=1,\n",
    "    #                 verbose=True,\n",
    "    #             ),\n",
    "    #             {\"C\": [0.1, 0.01, 0.001, 1]},\n",
    "    #             cv=5,\n",
    "    #         ),\n",
    "    #         \"LogReg\",\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    def do_split(split, X, Y, f, model, subjects):\n",
    "        train, test = split\n",
    "        train, test = subjects[train], subjects[test]\n",
    "        train = Y[\"subject\"].isin(train)\n",
    "        test = Y[\"subject\"].isin(test)\n",
    "        Y_train, labels_dict = preprocess_label(Y[train], return_dict=True)\n",
    "        # XXX: FIX ME\n",
    "        # Y_test = preprocess_label(Y[test], use_dict=labels_dict)\n",
    "        Y_test = preprocess_label(Y[test])\n",
    "        X_train = np.array(X)[train.values]\n",
    "        X_test = np.array(X)[test.values]\n",
    "        clf = AugmentedClassifier(model, f)\n",
    "        clf.fit(X_train, Y_train)\n",
    "        score_split = clf.score(X_test, Y_test)\n",
    "        return score_split\n",
    "\n",
    "    scores = []\n",
    "    for model, name in models:\n",
    "        subjects = labels[\"subject\"].unique()\n",
    "        sf = ShuffleSplit(\n",
    "            n_splits=n_splits, train_size=train_size, random_state=0\n",
    "        )\n",
    "        scores_split = Parallel(verbose=100, n_jobs=n_jobs)(\n",
    "            delayed(do_split)(split, images_path, labels, f, model, subjects)\n",
    "            for split in sf.split(range(len(subjects)))\n",
    "        )\n",
    "        for i_split, score_split in enumerate(scores_split):\n",
    "            scores.append((method_name, name, score_split, i_split))\n",
    "\n",
    "    scores = pd.DataFrame(\n",
    "        scores, columns=[\"method_name\", \"algo\", \"score\", \"split\"]\n",
    "    )\n",
    "    scores.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dcba691",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda X, Y: transform_based_augmentation(all_augs, mask, Z_inv, X, labels=Y, nb_fakes=200, n_jobs=-1, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f336c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea105ff3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 72 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "train_size = 8\n",
    "\n",
    "do_classif(\n",
    "    data[\"images\"],\n",
    "    labels,\n",
    "    f,\n",
    "    method_name=\"all_augs\",\n",
    "    filename=\"results/hcp_all_augs.csv\",\n",
    "    train_size=train_size,\n",
    "    n_splits=10,\n",
    "    n_jobs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dbef3ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-78-68a99804cc3d>\u001b[0m(77)\u001b[0;36mdo_split\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     75 \u001b[0;31m        \u001b[0;31m# Y_test = preprocess_label(Y[test], use_dict=labels_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     76 \u001b[0;31m        \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 77 \u001b[0;31m        \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     78 \u001b[0;31m        \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     79 \u001b[0;31m        \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAugmentedClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> type(X)\n",
      "<class 'list'>\n",
      "ipdb> train\n",
      "0     True\n",
      "1     True\n",
      "2    False\n",
      "3     True\n",
      "4     True\n",
      "5     True\n",
      "6     True\n",
      "7     True\n",
      "8    False\n",
      "9     True\n",
      "Name: subject, dtype: bool\n",
      "ipdb> X[train.values]\n",
      "*** TypeError: only integer scalar arrays can be converted to a scalar index\n",
      "ipdb> np.array(X)[train.values]\n",
      "array(['/storage/store2/data/neurovault/collection_4337/image_84774.nii.gz',\n",
      "       '/storage/store2/data/neurovault/collection_4337/image_86340.nii.gz',\n",
      "       '/storage/store2/data/neurovault/collection_4337/image_72795.nii.gz',\n",
      "       '/storage/store2/data/neurovault/collection_4337/image_78481.nii.gz',\n",
      "       '/storage/store2/data/neurovault/collection_4337/image_76410.nii.gz',\n",
      "       '/storage/store2/data/neurovault/collection_4337/image_92718.nii.gz',\n",
      "       '/storage/store2/data/neurovault/collection_4337/image_85777.nii.gz',\n",
      "       '/storage/store2/data/neurovault/collection_4337/image_82472.nii.gz'],\n",
      "      dtype='<U66')\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec18507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
